{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'utility'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-1f4961147b04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msequence\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpad_sequences\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mutility\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[1;33m*\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mwarnings\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'utility'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense,RNN,Flatten,Softmax\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from utility import *\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import pickle\n",
    "from nltk.translate.gleu_score import sentence_gleu\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading trained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = encoder_decoder(inp_vocab_size,out_vocab_size,embedding_dim,lstm_size,input_length,batch_size,score_fun,att_units)\n",
    "model.compile(optimizer=optimizer,loss=loss_function)\n",
    "model.train_on_batch([incorr_train,corr_train_inp],corr_train_out)\n",
    "model.load_weights('attention90low')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function1(input_sentence):\n",
    "    \"\"\"\n",
    "    this function takes incorrect sentence as input and returns correct sentence as output\n",
    "    \"\"\"\n",
    "    \n",
    "    ## load saved tokenizer\n",
    "    with open(\"tokenizer_corr_out_word_attention.pickle\",\"rb\") as temp3:\n",
    "        tokenizer_corr_out = pickle.load(temp3)\n",
    "    with open(\"tokenizer_incorr_word_attention.pickle\",\"rb\") as temp1:\n",
    "        tokenizer_incorr = pickle.load(temp1)\n",
    "    \n",
    "    ## store word and tokens, as to use it during inference\n",
    "    corr_dict = tokenizer_corr_out.word_index\n",
    "    inv_corr = {v: k for k, v in corr_dict.items()}\n",
    "\n",
    "    ## tokenize input sentence using tokenizer\n",
    "    input_sentence = tokenizer_incorr.texts_to_sequences([input_sentence])[0]\n",
    "    \n",
    "    ## initialize hidden states and cell stated of dencoder\n",
    "    initial_hidden_state = tf.zeros([1,64])\n",
    "    initial_cell_state = tf.zeros([1,64])\n",
    "    encoder_initial_state = [initial_hidden_state,initial_cell_state]\n",
    "    input_sentence = tf.keras.preprocessing.sequence.pad_sequences([input_sentence],maxlen=25,padding='post')\n",
    "    input_sentence = input_sentence[0]\n",
    "    \n",
    "    ## pass input to encoder and get encoder output, hidden and cell state\n",
    "    enc_output, enc_state_h, enc_state_c = model.layers[0](np.expand_dims(input_sentence,0),encoder_initial_state)\n",
    "    \n",
    "    all = []\n",
    "    pred = []\n",
    "    sentence = []\n",
    "    ## token for start of sentence is 1, so we will pass this to decoder\n",
    "    cur_vec = np.ones((1, 1),dtype='int')\n",
    "    \n",
    "    ## iterate over input sentence\n",
    "    for i in range(26):\n",
    "        \n",
    "        infe_output,deco_state_h, deco_state_c,attention_weights,context_vector = model.layers[1].onestepdecoder(cur_vec, enc_output,enc_state_h, enc_state_c)\n",
    "        \n",
    "        ## replace previous hidden and cell state with curent hidden and cell state\n",
    "        enc_state_h, enc_state_c = deco_state_h, deco_state_c\n",
    "        \n",
    "        ## reshaping current word \n",
    "        cur_vec = np.reshape(np.argmax(infe_output), (1, 1))\n",
    "        \n",
    "        ## if model predicted @(end of sentence) break loop\n",
    "        if inv_corr[cur_vec[0][0]] == '@':\n",
    "            break\n",
    "        temp1 = np.array(infe_output[0])\n",
    "        \n",
    "        ## store probablities ate each time stamp in a list\n",
    "        all.append(temp1)\n",
    "        \n",
    "        ## store current output tokens in a list \n",
    "        pred.append(cur_vec[0][0])\n",
    "    for i in pred:\n",
    "        sentence.append(inv_corr[i])\n",
    "\n",
    "    y = []\n",
    "    for i in all:\n",
    "        y.append(tf.nn.softmax(i).numpy())\n",
    "    \n",
    "    ## use beam search decoder \n",
    "    z = beam_search_decoder(y, top_k = 1)## \n",
    "    results = []\n",
    "    for i in z:\n",
    "        sentence = []\n",
    "        for j in i[0]:\n",
    "        \n",
    "            sentence.append(inv_corr[j])\n",
    "        results.append(\" \".join(sentence))\n",
    "    return  results[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Testing function 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = function1(\"But today are wednesday\")\n",
    "print(\"Correct grammatical sentence is -\",result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### defining function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def function2(data):\n",
    "    \n",
    "    \"\"\"\n",
    "    this function takes input data file as input and prints average GLUE score\n",
    "    \"\"\"\n",
    "    test = pd.read_csv(data)\n",
    "    gleu_score_test = 0\n",
    "    length = test.shape[0]\n",
    "\n",
    "    for i in range(length):\n",
    "        reference = test[\"correct\"].values[i:i+1][0].split()\n",
    "        predicted = function1(test[\"incorrect\"].values[i:i+1][0])\n",
    "        split_predicted = [j.split() for j in predicted]\n",
    "        gleu_score_test = gleu_score_test + sentence_gleu(split_predicted,reference)\n",
    "    print(\"Final GLEU Score on Test data are\",gleu_score_test/length)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### testing function 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function2(\"test_attention.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
